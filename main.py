# main.py
import os
import sys

import streamlit as st
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

from llms.deepseek import DeepSeekLLM  # âœ… å½“å‰ç”¨ DeepSeek
# from llms.bili_custom import BilibiliChatLLM  # âœ… åé¢åˆ‡æ¢å…¬å¸ LLM æ—¶æ”¹è¿™é‡Œ


@st.cache_resource
def build_vector_store():
    loader = TextLoader("files/example.md", encoding="utf-8")
    docs = loader.load()
    chunks = CharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5")
    return FAISS.from_documents(chunks, embeddings)


def get_api_key(filepath="api.txt") -> str:
    if not os.path.exists(filepath):
        sys.exit(f"âŒ API key file '{filepath}' not found.")

    with open(filepath, "r", encoding="utf-8") as f:
        key = f.read().strip()
        if not key:
            sys.exit("âŒ API key is empty.")
        return key


if __name__ == '__main__':
    st.set_page_config(page_title="RAG Demo", layout="wide")
    st.title("ğŸ“š RAG æœ¬åœ°çŸ¥è¯†åº“é—®ç­”")

    db = build_vector_store()
    retriever = db.as_retriever(search_kwargs={"k": 3})
    llm = DeepSeekLLM(api_key=get_api_key())  # ğŸ‘ˆ å¡«ä½ çš„ DeepSeek Key
    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
    query = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜")
    if query:
        with st.spinner("æ€è€ƒä¸­..."):
            result = qa.run(query)
        st.success(result)
